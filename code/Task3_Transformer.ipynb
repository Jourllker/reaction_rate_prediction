{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 安装环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchtext\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 对数据处理进行初步尝试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from typing import List, Tuple\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer，鉴于SMILES的特性，这里需要自己定义tokenizer和vocab\n",
    "# 这里直接将smiles str按字符拆分，并替换为词汇表中的序号\n",
    "class Smiles_tokenizer():\n",
    "    def __init__(self, pad_token, regex, vocab_file, max_length):\n",
    "        self.pad_token = pad_token\n",
    "        self.regex = regex\n",
    "        self.vocab_file = vocab_file\n",
    "        self.max_length = max_length\n",
    "\n",
    "        with open(self.vocab_file, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "        lines = [line.strip(\"\\n\") for line in lines]\n",
    "        vocab_dic = {}\n",
    "        for index, token in enumerate(lines):\n",
    "            vocab_dic[token] = index\n",
    "        self.vocab_dic = vocab_dic\n",
    "\n",
    "    def _regex_match(self, smiles):\n",
    "        regex_string = r\"(\" + self.regex + r\"|\"\n",
    "        regex_string += r\".)\"\n",
    "        prog = re.compile(regex_string)\n",
    "\n",
    "        tokenised = []\n",
    "        for smi in smiles:\n",
    "            tokens = prog.findall(smi)\n",
    "            if len(tokens) > self.max_length:\n",
    "                tokens = tokens[:self.max_length]\n",
    "            tokenised.append(tokens) # 返回一个所有的字符串列表\n",
    "        return tokenised\n",
    "    \n",
    "    def tokenize(self, smiles):\n",
    "        tokens = self._regex_match(smiles)\n",
    "        # 添加上表示开始和结束的token：<cls>, <end>\n",
    "        tokens = [[\"<CLS>\"] + token + [\"<SEP>\"] for token in tokens]\n",
    "        tokens = self._pad_seqs(tokens, self.pad_token)\n",
    "        token_idx = self._pad_token_to_idx(tokens)\n",
    "        return tokens, token_idx\n",
    "\n",
    "    def _pad_seqs(self, seqs, pad_token):\n",
    "        pad_length = max([len(seq) for seq in seqs])\n",
    "        padded = [seq + ([pad_token] * (pad_length - len(seq))) for seq in seqs]\n",
    "        return padded\n",
    "\n",
    "    def _pad_token_to_idx(self, tokens):\n",
    "        idx_list = []\n",
    "        new_vocab = []\n",
    "        for token in tokens:\n",
    "            tokens_idx = []\n",
    "            for i in token:\n",
    "                if i in self.vocab_dic.keys():\n",
    "                    tokens_idx.append(self.vocab_dic[i])\n",
    "                else:\n",
    "                    new_vocab.append(i)\n",
    "                    self.vocab_dic[i] = max(self.vocab_dic.values()) + 1\n",
    "                    tokens_idx.append(self.vocab_dic[i])\n",
    "            idx_list.append(tokens_idx)\n",
    "\n",
    "        with open(\"../new_vocab_list.txt\", \"a\") as f:\n",
    "            for i in new_vocab:\n",
    "                f.write(i)\n",
    "                f.write(\"\\n\")\n",
    "\n",
    "        return idx_list\n",
    "\n",
    "    def _save_vocab(self, vocab_path):\n",
    "        with open(vocab_path, \"w\") as f:\n",
    "            for i in self.vocab_dic.keys():\n",
    "                f.write(i)\n",
    "                f.write(\"\\n\")\n",
    "        print(\"update new vocab!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理数据\n",
    "\n",
    "def read_data(file_path, train=True):\n",
    "    df = pd.read_csv(file_path)\n",
    "    reactant1 = df[\"Reactant1\"].tolist()\n",
    "    reactant2 = df[\"Reactant2\"].tolist()\n",
    "    product = df[\"Product\"].tolist()\n",
    "    additive = df[\"Additive\"].tolist()\n",
    "    solvent = df[\"Solvent\"].tolist()\n",
    "    if train:\n",
    "        react_yield = df[\"Yield\"].tolist()\n",
    "    else:\n",
    "        react_yield = [0 for i in range(len(reactant1))]\n",
    "    \n",
    "    # 将reactant\\additive\\solvent拼到一起，之间用.分开。product也拼到一起，用>>分开\n",
    "    input_data_list = []\n",
    "    for react1, react2, prod, addi, sol in zip(reactant1, reactant2, product, additive, solvent):\n",
    "        # input_info = \".\".join([react1, react2, addi, sol])\n",
    "        input_info = \".\".join([react1, react2])\n",
    "        input_info = \">\".join([input_info, prod])\n",
    "        input_data_list.append(input_info)\n",
    "    output = [(react, y) for react, y in zip(input_data_list, react_yield)]\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义数据集\n",
    "class ReactionDataset(Dataset):\n",
    "    def __init__(self, data: List[Tuple[List[str], float]]):\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    REGEX = r\"\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\\\\\|\\/|:|~|@|\\?|>|\\*|\\$|\\%[0-9]{2}|[0-9]\"\n",
    "    tokenizer = Smiles_tokenizer(\"<PAD>\", REGEX, \"../vocab_full_new.txt\", 300)\n",
    "    smi_list = []\n",
    "    yield_list = []\n",
    "    for i in batch:\n",
    "        smi_list.append(i[0])\n",
    "        yield_list.append(i[1])\n",
    "    tokenizer_batch = torch.tensor(tokenizer.tokenize(smi_list)[1])\n",
    "    yield_list = torch.tensor(yield_list)\n",
    "    return tokenizer_batch, yield_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型\n",
    "'''\n",
    "直接采用一个transformer encoder model就好了\n",
    "'''\n",
    "class TransformerEncoderModel(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, num_heads, fnn_dim, num_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, d_model)\n",
    "        self.layerNorm = nn.LayerNorm(d_model)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, \n",
    "                                                        nhead=num_heads, \n",
    "                                                        dim_feedforward=fnn_dim,\n",
    "                                                        dropout=dropout,\n",
    "                                                        batch_first=True,\n",
    "                                                        norm_first=True # pre-layernorm\n",
    "                                                        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, \n",
    "                                                         num_layers=num_layers,\n",
    "                                                         norm=self.layerNorm)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.lc = nn.Sequential(nn.Linear(d_model, 256),\n",
    "                                nn.Sigmoid(),\n",
    "                                nn.Linear(256, 96),\n",
    "                                nn.Sigmoid(),\n",
    "                                nn.Linear(96, 1))\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src shape: [batch_size, src_len]\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        # embedded shape: [batch_size, src_len, d_model]\n",
    "        outputs = self.transformer_encoder(embedded)\n",
    "        # outputs shape: [batch_size, src_len, d_model]\n",
    "\n",
    "        # fisrt\n",
    "        z = outputs[:,0,:]\n",
    "        # z = torch.sum(outputs, dim=1)\n",
    "        # print(z)\n",
    "        # z shape: [bs, d_model]\n",
    "        outputs = self.lc(z)\n",
    "        # print(outputs)\n",
    "        # outputs shape: [bs, 1]\n",
    "        return outputs.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch, start_lr):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = start_lr * (0.1 ** (epoch // 3))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练\n",
    "def train():\n",
    "    ## super param\n",
    "    N = 10  #int / int(len(dataset) * 1)  # 或者你可以设置为数据集大小的一定比例，如 int(len(dataset) * 0.1)\n",
    "    INPUT_DIM = 292 # src length\n",
    "    D_MODEL = 512\n",
    "    NUM_HEADS = 4\n",
    "    FNN_DIM = 1024\n",
    "    NUM_LAYERS = 4\n",
    "    DROPOUT = 0.2\n",
    "    CLIP = 1 # CLIP value\n",
    "    N_EPOCHS = 40\n",
    "    LR = 1e-4\n",
    "    \n",
    "    start_time = time.time()  # 开始计时\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # device = 'cpu'\n",
    "    data = read_data(\"../dataset/round1_train_data.csv\")\n",
    "    dataset = ReactionDataset(data)\n",
    "    subset_indices = list(range(N))\n",
    "    subset_dataset = Subset(dataset, subset_indices)\n",
    "    train_loader = DataLoader(dataset, batch_size=128, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "    model = TransformerEncoderModel(INPUT_DIM, D_MODEL, NUM_HEADS, FNN_DIM, NUM_LAYERS, DROPOUT)\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    best_valid_loss = 10\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        epoch_loss = 0\n",
    "        # adjust_learning_rate(optimizer, epoch, LR) # 动态调整学习率\n",
    "        for i, (src, y) in enumerate(train_loader):\n",
    "            src, y = src.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(src)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.detach().item()\n",
    "            \n",
    "            if i % 50 == 0:\n",
    "                print(f'Step: {i} | Train Loss: {epoch_loss:.4f}')\n",
    "                \n",
    "        scheduler.step(loss_in_a_epoch)\n",
    "        loss_in_a_epoch = epoch_loss / len(train_loader)\n",
    "        print(f'Epoch: {epoch+1:02} | Train Loss: {loss_in_a_epoch:.3f}')\n",
    "        if loss_in_a_epoch < best_valid_loss:\n",
    "            best_valid_loss = loss_in_a_epoch\n",
    "            # 在训练循环结束后保存模型\n",
    "            torch.save(model.state_dict(), '../model/transformer.pth')\n",
    "    end_time = time.time()  # 结束计时\n",
    "    # 计算并打印运行时间\n",
    "    elapsed_time_minute = (end_time - start_time)/60\n",
    "    print(f\"Total running time: {elapsed_time_minute:.2f} minutes\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成结果文件\n",
    "def predicit_and_make_submit_file(model_file, output_file):\n",
    "    INPUT_DIM = 292 # src length\n",
    "    D_MODEL = 512\n",
    "    NUM_HEADS = 4\n",
    "    FNN_DIM = 1024\n",
    "    NUM_LAYERS = 4\n",
    "    DROPOUT = 0.2\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    test_data = read_data(\"../dataset/round1_test_data.csv\", train=False)\n",
    "    test_dataset = ReactionDataset(test_data)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, collate_fn=collate_fn) \n",
    "\n",
    "    model = TransformerEncoderModel(INPUT_DIM, D_MODEL, NUM_HEADS, FNN_DIM, NUM_LAYERS, DROPOUT).to(device)\n",
    "    # 加载最佳模型\n",
    "    model.load_state_dict(torch.load(model_file))\n",
    "    model.eval()\n",
    "    output_list = []\n",
    "    for i, (src, y) in enumerate(test_loader):\n",
    "        src = src.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(src)\n",
    "            output_list += output.detach().tolist()\n",
    "    ans_str_lst = ['rxnid,Yield']\n",
    "    for idx,y in enumerate(output_list):\n",
    "        ans_str_lst.append(f'test{idx+1},{y:.4f}')\n",
    "    with open(output_file,'w') as fw:\n",
    "        fw.writelines('\\n'.join(ans_str_lst))\n",
    "    \n",
    "predicit_and_make_submit_file(\"../model/transformer.pth\",\n",
    "                              \"../output/result.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
