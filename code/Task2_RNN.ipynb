{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义RNN模型\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, num_embed, input_size, hidden_size, output_size, num_layers, dropout, device):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.embed = nn.Embedding(num_embed, input_size)\n",
    "        # self.rnn = nn.RNN(input_size, hidden_size, num_layers=num_layers, \n",
    "        #                   batch_first=True, dropout=dropout, bidirectional=True, device=device)\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers=num_layers, \n",
    "                          batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        self.fc = nn.Sequential(nn.Linear(2 * num_layers * hidden_size, output_size),\n",
    "                                nn.Sigmoid(),\n",
    "                                nn.Linear(output_size, 1),\n",
    "                                nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x : [bs, seq_len]\n",
    "        x = self.embed(x)\n",
    "        # x : [bs, seq_len, input_size]\n",
    "        _, hn = self.rnn(x) # hn : [2*num_layers, bs, h_dim]\n",
    "        hn = hn.transpose(0,1)\n",
    "        z = hn.reshape(hn.shape[0], -1)\n",
    "        output = self.fc(z).squeeze(-1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "## 数据处理部分\n",
    "# tokenizer，鉴于SMILES的特性，这里需要自己定义tokenizer和vocab\n",
    "# 这里直接将smiles str按字符拆分，并替换为词汇表中的序号\n",
    "class Smiles_tokenizer():\n",
    "    def __init__(self, pad_token, regex, vocab_file, max_length):\n",
    "        self.pad_token = pad_token\n",
    "        self.regex = regex\n",
    "        self.vocab_file = vocab_file\n",
    "        self.max_length = max_length\n",
    "\n",
    "        with open(self.vocab_file, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "        lines = [line.strip(\"\\n\") for line in lines]\n",
    "        vocab_dic = {}\n",
    "        for index, token in enumerate(lines):\n",
    "            vocab_dic[token] = index\n",
    "        self.vocab_dic = vocab_dic\n",
    "\n",
    "    def _regex_match(self, smiles):\n",
    "        regex_string = r\"(\" + self.regex + r\"|\"\n",
    "        regex_string += r\".)\"\n",
    "        prog = re.compile(regex_string)\n",
    "\n",
    "        tokenised = []\n",
    "        for smi in smiles:\n",
    "            tokens = prog.findall(smi)\n",
    "            if len(tokens) > self.max_length:\n",
    "                tokens = tokens[:self.max_length]\n",
    "            tokenised.append(tokens) # 返回一个所有的字符串列表\n",
    "        return tokenised\n",
    "    \n",
    "    def tokenize(self, smiles):\n",
    "        tokens = self._regex_match(smiles)\n",
    "        # 添加上表示开始和结束的token：<cls>, <end>\n",
    "        tokens = [[\"<CLS>\"] + token + [\"<SEP>\"] for token in tokens]\n",
    "        tokens = self._pad_seqs(tokens, self.pad_token)\n",
    "        token_idx = self._pad_token_to_idx(tokens)\n",
    "        return tokens, token_idx\n",
    "\n",
    "    def _pad_seqs(self, seqs, pad_token):\n",
    "        pad_length = max([len(seq) for seq in seqs])\n",
    "        padded = [seq + ([pad_token] * (pad_length - len(seq))) for seq in seqs]\n",
    "        return padded\n",
    "\n",
    "    def _pad_token_to_idx(self, tokens):\n",
    "        idx_list = []\n",
    "        for token in tokens:\n",
    "            tokens_idx = []\n",
    "            for i in token:\n",
    "                if i in self.vocab_dic.keys():\n",
    "                    tokens_idx.append(self.vocab_dic[i])\n",
    "                else:\n",
    "                    self.vocab_dic[i] = max(self.vocab_dic.values()) + 1\n",
    "                    tokens_idx.append(self.vocab_dic[i])\n",
    "            idx_list.append(tokens_idx)\n",
    "        \n",
    "        return idx_list\n",
    "\n",
    "# 读数据并处理\n",
    "def read_data(file_path, train=True):\n",
    "    df = pd.read_csv(file_path)\n",
    "    reactant1 = df[\"Reactant1\"].tolist()\n",
    "    reactant2 = df[\"Reactant2\"].tolist()\n",
    "    product = df[\"Product\"].tolist()\n",
    "    additive = df[\"Additive\"].tolist()\n",
    "    solvent = df[\"Solvent\"].tolist()\n",
    "    if train:\n",
    "        react_yield = df[\"Yield\"].tolist()\n",
    "    else:\n",
    "        react_yield = [0 for i in range(len(reactant1))]\n",
    "    \n",
    "    # 将reactant\\additive\\solvent拼到一起，之间用.分开。product也拼到一起，用>分开\n",
    "    '''\n",
    "    input_data_list = []\n",
    "    for react1, react2, prod, addi, sol in zip(reactant1, reactant2, product, additive, solvent):\n",
    "        input_info = \".\".join([react1, react2, addi, sol])\n",
    "        input_info = \">\".join([input_info, prod])\n",
    "        input_data_list.append(input_info)\n",
    "    output = [(react, y) for react, y in zip(input_data_list, react_yield)]\n",
    "    '''\n",
    "    # 将reactant拼到一起，之间用.分开。product也拼到一起，用>分开\n",
    "    input_data_list = []\n",
    "    for react1, react2, prod, addi, sol in zip(reactant1, reactant2, product, additive, solvent):\n",
    "        input_info = \".\".join([react1, react2])\n",
    "        input_info = \">\".join([input_info, prod])\n",
    "        input_data_list.append(input_info)\n",
    "    output = [(react, y) for react, y in zip(input_data_list, react_yield)]\n",
    "\n",
    "    # # 统计seq length\n",
    "    # seq_length = [len(i[0]) for i in output]\n",
    "    # seq_length_400 = [len(i[0]) for i in output if len(i[0])>200]\n",
    "    # print(len(seq_length_400) / len(seq_length))\n",
    "    # seq_length.sort(reverse=True)\n",
    "    # plt.plot(range(len(seq_length)), seq_length)\n",
    "    # plt.title(\"templates frequence\")\n",
    "    # plt.show()\n",
    "    return output\n",
    "\n",
    "class ReactionDataset(Dataset):\n",
    "    def __init__(self, data: List[Tuple[List[str], float]]):\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # input_info, react_yeild = self.data[idx]\n",
    "        # return input_info, react_yeild\n",
    "        return self.data[idx]\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    REGEX = r\"\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\\\\\|\\/|:|~|@|\\?|>|\\*|\\$|\\%[0-9]{2}|[0-9]\"\n",
    "    tokenizer = Smiles_tokenizer(\"<PAD>\", REGEX, \"../vocab_full.txt\", max_length=200)\n",
    "    smi_list = []\n",
    "    yield_list = []\n",
    "    for i in batch:\n",
    "        smi_list.append(i[0])\n",
    "        yield_list.append(i[1])\n",
    "    tokenizer_batch = torch.tensor(tokenizer.tokenize(smi_list)[1])\n",
    "    yield_list = torch.tensor(yield_list)\n",
    "    return tokenizer_batch, yield_list\n",
    "\n",
    "\n",
    "# read_data(\"../dataset/round1_train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    ## super param\n",
    "    N = 10  #int / int(len(dataset) * 1)  # 或者你可以设置为数据集大小的一定比例，如 int(len(dataset) * 0.1)\n",
    "    NUM_EMBED = 300 # nn.Embedding()\n",
    "    INPUT_SIZE = 200 # src length\n",
    "    HIDDEN_SIZE = 512\n",
    "    OUTPUT_SIZE = 512\n",
    "    NUM_LAYERS = 10\n",
    "    DROPOUT = 0.5\n",
    "    CLIP = 1 # CLIP value\n",
    "    N_EPOCHS = 10\n",
    "    LR = 0.001\n",
    "    \n",
    "    start_time = time.time()  # 开始计时\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # device = 'cpu'\n",
    "    data = read_data(\"../dataset/round1_train_data.csv\")\n",
    "    dataset = ReactionDataset(data)\n",
    "    subset_indices = list(range(N))\n",
    "    subset_dataset = Subset(dataset, subset_indices)\n",
    "    train_loader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "    model = RNNModel(NUM_EMBED, INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE, NUM_LAYERS, DROPOUT, device).to(device)\n",
    "    model.train()\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "    # criterion = nn.MSELoss()\n",
    "    criterion = nn.L1Loss() # MAE\n",
    "\n",
    "    best_loss = 10\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        epoch_loss = 0\n",
    "        for i, (src, y) in enumerate(train_loader):\n",
    "            src, y = src.to(device), y.to(device)\n",
    "            # print(src.shape)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(src)\n",
    "            loss = criterion(output, y)\n",
    "            with open(\"loss.txt\", \"a\") as f:\n",
    "                f.write(str(loss.detach().cpu()))\n",
    "                f.write('\\n')\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "            optimizer.step()\n",
    "            # raise KeyError\n",
    "            epoch_loss += loss.item()\n",
    "            loss_in_a_epoch = epoch_loss / len(train_loader)\n",
    "        print(f'Epoch: {epoch+1:02} | Train Loss: {loss_in_a_epoch:.3f}')\n",
    "        if loss_in_a_epoch < best_loss:\n",
    "            # 在训练循环结束后保存模型\n",
    "            torch.save(model.state_dict(), '../model/RNN-1.pth')\n",
    "    end_time = time.time()  # 结束计时\n",
    "    # 计算并打印运行时间\n",
    "    elapsed_time_minute = (end_time - start_time)/60\n",
    "    print(f\"Total running time: {elapsed_time_minute:.2f} minutes\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成结果文件\n",
    "def predicit_and_make_submit_file(model_file, output_file):\n",
    "    NUM_EMBED = 300\n",
    "    INPUT_SIZE = 300\n",
    "    HIDDEN_SIZE = 512\n",
    "    OUTPUT_SIZE = 512\n",
    "    NUM_LAYERS = 3\n",
    "    DROPOUT = 0.5\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # device = 'cpu'\n",
    "    test_data = read_data(\"../dataset/round1_test_data.csv\", train=False)\n",
    "    test_dataset = ReactionDataset(test_data)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn) \n",
    "\n",
    "    model = RNNModel(NUM_EMBED, INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE, NUM_LAYERS, DROPOUT, device).to(device)\n",
    "    # 加载最佳模型\n",
    "    model.load_state_dict(torch.load(model_file))\n",
    "    model.eval()\n",
    "    output_list = []\n",
    "    src_list = []\n",
    "    for i, (src, y) in enumerate(test_loader):\n",
    "        src, y = src.to(device), y.to(device)\n",
    "        print(src)\n",
    "        src_list.append(src)\n",
    "        output = model(src)\n",
    "        output_list += output.detach().tolist()\n",
    "        print(output_list)\n",
    "        raise KeyError\n",
    "    # print(len(output_list))\n",
    "    ans_str_lst = ['rxnid,Yield']\n",
    "    # for idx,y in enumerate(output_list):\n",
    "    #     ans_str_lst.append(f'test{idx+1},{y:.4f}')\n",
    "    for idx,(y,s) in enumerate(zip(output_list, )):\n",
    "        ans_str_lst.append(f'test{idx+1},{y:.4f}')\n",
    "    with open(output_file,'w') as fw:\n",
    "        fw.writelines('\\n'.join(ans_str_lst))\n",
    "    \n",
    "predicit_and_make_submit_file(\"../model/RNN.pth\",\n",
    "                              \"../output/RNN_submit.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usual_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
