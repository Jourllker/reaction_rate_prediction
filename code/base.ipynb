{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 对数据处理进行初步尝试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from typing import List, Tuple\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer，鉴于SMILES的特性，这里需要自己定义tokenizer和vocab\n",
    "# 这里直接将smiles str按字符拆分，并替换为词汇表中的序号\n",
    "class Smiles_tokenizer():\n",
    "    def __init__(self, pad_token, regex, vocab_file, max_length):\n",
    "        self.pad_token = pad_token\n",
    "        self.regex = regex\n",
    "        self.vocab_file = vocab_file\n",
    "        self.max_length = max_length\n",
    "\n",
    "        with open(self.vocab_file, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "        lines = [line.strip(\"\\n\") for line in lines]\n",
    "        vocab_dic = {}\n",
    "        for index, token in enumerate(lines):\n",
    "            vocab_dic[token] = index\n",
    "        self.vocab_dic = vocab_dic\n",
    "\n",
    "    def _regex_match(self, smiles):\n",
    "        regex_string = r\"(\" + self.regex + r\"|\"\n",
    "        regex_string += r\".)\"\n",
    "        prog = re.compile(regex_string)\n",
    "\n",
    "        tokenised = []\n",
    "        for smi in smiles:\n",
    "            tokens = prog.findall(smi)\n",
    "            if len(tokens) > self.max_length:\n",
    "                tokens = tokens[:self.max_length]\n",
    "            tokenised.append(tokens) # 返回一个所有的字符串列表\n",
    "        return tokenised\n",
    "    \n",
    "    def tokenize(self, smiles):\n",
    "        tokens = self._regex_match(smiles)\n",
    "        # 添加上表示开始和结束的token：<cls>, <end>\n",
    "        tokens = [[\"<CLS>\"] + token + [\"<SEP>\"] for token in tokens]\n",
    "        tokens = self._pad_seqs(tokens, self.pad_token)\n",
    "        token_idx = self._pad_token_to_idx(tokens)\n",
    "        return tokens, token_idx\n",
    "\n",
    "    def _pad_seqs(self, seqs, pad_token):\n",
    "        pad_length = max([len(seq) for seq in seqs])\n",
    "        padded = [seq + ([pad_token] * (pad_length - len(seq))) for seq in seqs]\n",
    "        return padded\n",
    "\n",
    "    def _pad_token_to_idx(self, tokens):\n",
    "        idx_list = []\n",
    "        for token in tokens:\n",
    "            tokens_idx = []\n",
    "            for i in token:\n",
    "                if i in self.vocab_dic.keys():\n",
    "                    tokens_idx.append(self.vocab_dic[i])\n",
    "                else:\n",
    "                    self.vocab_dic[i] = max(self.vocab_dic.values()) + 1\n",
    "                    tokens_idx.append(self.vocab_dic[i])\n",
    "            idx_list.append(tokens_idx)\n",
    "        \n",
    "        return idx_list\n",
    "\n",
    "# REGEX = r\"\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\\\\\|\\/|:|~|@|\\?|>|\\*|\\$|\\%[0-9]{2}|[0-9]\"\n",
    "# tokenizer = Smiles_tokenizer(\"<PAD>\", REGEX, \"../vocab_full.txt\", 10)\n",
    "\n",
    "# # 注意，这里一定要输入tuple（smiles1, smiles2）\n",
    "# res = tokenizer.tokenize(('[PH+][CCL2+]Clc1c(Cl)ccc(c1)[C@@H](C)[C@H](N=[N+]=[N-])C(=O)OC.C>C=O', \n",
    "#                         'Cc1cc(Br)c2c(c1)N(C(=O)O'))\n",
    "# # # res2 = tokenizer.tokenize(('[PH+][CCL2+]Clc1c(Cl)ccc(c1)[C@@H](C)[C@H](N=[N+]=[N-])C(=O)OC.C>C=O', \n",
    "# # #                           'Cc1cc(Br)c2c(c1)N(C(=O)O'))\n",
    "# print(res[0])\n",
    "# print(res[1])\n",
    "# # print(res2[0])\n",
    "# print(res2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理数据\n",
    "def read_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    reactant1 = df[\"Reactant1\"].tolist()\n",
    "    reactant2 = df[\"Reactant2\"].tolist()\n",
    "    product = df[\"Product\"].tolist()\n",
    "    additive = df[\"Additive\"].tolist()\n",
    "    solvent = df[\"Solvent\"].tolist()\n",
    "    react_yield = df[\"Yield\"].tolist()\n",
    "    \n",
    "    # 将reactant\\additive\\solvent拼到一起，之间用.分开。product也拼到一起，用>>分开\n",
    "    input_data_list = []\n",
    "    for react1, react2, prod, addi, sol in zip(reactant1, reactant2, product, additive, solvent):\n",
    "        input_info = \".\".join([react1, react2, addi, sol])\n",
    "        input_info = \">\".join([input_info, prod])\n",
    "        input_data_list.append(input_info)\n",
    "    output = [(react, y) for react, y in zip(input_data_list, react_yield)]\n",
    "    return output\n",
    "# read_data(\"../dataset/train_data_demo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义数据集\n",
    "class ReactionDataset(Dataset):\n",
    "    def __init__(self, data: List[Tuple[List[str], float]], SMILES_tokenizer):\n",
    "        self.data = data\n",
    "        self.smiles_tokenizer = SMILES_tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_info, react_yeild = self.data[idx]\n",
    "        # input_info = self.smiles_tokenizer.tokenize(input_info) # 对SMILES进行tokenize\n",
    "\n",
    "        return input_info, react_yeild\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    REGEX = r\"\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\\\\\|\\/|:|~|@|\\?|>|\\*|\\$|\\%[0-9]{2}|[0-9]\"\n",
    "    tokenizer = Smiles_tokenizer(\"<PAD>\", REGEX, \"../vocab_full.txt\", 10)\n",
    "    smi_list = []\n",
    "    yield_list = []\n",
    "    for i in batch:\n",
    "        smi_list.append(i[0])\n",
    "        yield_list.append(i[1])\n",
    "    tokenizer_batch = tokenizer.tokenize(smi_list)\n",
    "    return tokenizer_batch, yield_list\n",
    "\n",
    "# res1, res2 = collate_fn(('Clc1c(Cl)ccc(c1)[C@@H](C)[C@H](N=[N+]=[N-])C(=O)OC.C>C=O', 'Cc1cc(Br)c2c(c1)N(C(=O)O'))\n",
    "# print(res1)\n",
    "# print(res2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([['<CLS>', 'c', '1', 'c', 'c', 'c', '2', 'c', '(', 'c', '1', '<SEP>'], ['<CLS>', 'c', '1', 'c', 'c', 'c', '2', 'c', '(', 'c', '1', '<SEP>']], [[1, 7, 8, 7, 7, 7, 11, 7, 9, 7, 8, 3], [1, 7, 8, 7, 7, 7, 11, 7, 9, 7, 8, 3]])\n",
      "[0.85, 0.9]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(src)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(trg)\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data = read_data(\"../dataset/train_data_demo.csv\")\n",
    "# REGEX = r\"\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\\\\\|\\/|:|~|@|\\?|>|\\*|\\$|\\%[0-9]{2}|[0-9]\"\n",
    "# tokenizer = Smiles_tokenizer(\"<PAD>\", REGEX, \"../vocab_full.txt\") \n",
    "dataset = ReactionDataset(data, None)\n",
    "# 选择数据集的前N个样本进行训练\n",
    "N = 1  #int(len(dataset) * 1)  # 或者你可以设置为数据集大小的一定比例，如 int(len(dataset) * 0.1)\n",
    "subset_indices = list(range(3))\n",
    "subset_dataset = Subset(dataset, subset_indices)\n",
    "train_loader = DataLoader(subset_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "for i, (src, trg) in enumerate(train_loader):\n",
    "    print(src)\n",
    "    print(trg)\n",
    "    raise KeyError"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
