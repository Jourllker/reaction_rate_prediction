{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 安装环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchtext\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 对数据处理进行初步尝试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from typing import List, Tuple\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer，鉴于SMILES的特性，这里需要自己定义tokenizer和vocab\n",
    "# 这里直接将smiles str按字符拆分，并替换为词汇表中的序号\n",
    "class Smiles_tokenizer():\n",
    "    def __init__(self, pad_token, regex, vocab_file, max_length):\n",
    "        self.pad_token = pad_token\n",
    "        self.regex = regex\n",
    "        self.vocab_file = vocab_file\n",
    "        self.max_length = max_length\n",
    "\n",
    "        with open(self.vocab_file, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "        lines = [line.strip(\"\\n\") for line in lines]\n",
    "        vocab_dic = {}\n",
    "        for index, token in enumerate(lines):\n",
    "            vocab_dic[token] = index\n",
    "        self.vocab_dic = vocab_dic\n",
    "\n",
    "    def _regex_match(self, smiles):\n",
    "        regex_string = r\"(\" + self.regex + r\"|\"\n",
    "        regex_string += r\".)\"\n",
    "        prog = re.compile(regex_string)\n",
    "\n",
    "        tokenised = []\n",
    "        for smi in smiles:\n",
    "            tokens = prog.findall(smi)\n",
    "            if len(tokens) > self.max_length:\n",
    "                tokens = tokens[:self.max_length]\n",
    "            tokenised.append(tokens) # 返回一个所有的字符串列表\n",
    "        return tokenised\n",
    "    \n",
    "    def tokenize(self, smiles):\n",
    "        tokens = self._regex_match(smiles)\n",
    "        # 添加上表示开始和结束的token：<cls>, <end>\n",
    "        tokens = [[\"<CLS>\"] + token + [\"<SEP>\"] for token in tokens]\n",
    "        tokens = self._pad_seqs(tokens, self.pad_token)\n",
    "        token_idx = self._pad_token_to_idx(tokens)\n",
    "        return tokens, token_idx\n",
    "\n",
    "    def _pad_seqs(self, seqs, pad_token):\n",
    "        pad_length = max([len(seq) for seq in seqs])\n",
    "        padded = [seq + ([pad_token] * (pad_length - len(seq))) for seq in seqs]\n",
    "        return padded\n",
    "\n",
    "    def _pad_token_to_idx(self, tokens):\n",
    "        idx_list = []\n",
    "        for token in tokens:\n",
    "            tokens_idx = []\n",
    "            for i in token:\n",
    "                if i in self.vocab_dic.keys():\n",
    "                    tokens_idx.append(self.vocab_dic[i])\n",
    "                else:\n",
    "                    self.vocab_dic[i] = max(self.vocab_dic.values()) + 1\n",
    "                    tokens_idx.append(self.vocab_dic[i])\n",
    "            idx_list.append(tokens_idx)\n",
    "        \n",
    "        return idx_list\n",
    "\n",
    "# REGEX = r\"\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\\\\\|\\/|:|~|@|\\?|>|\\*|\\$|\\%[0-9]{2}|[0-9]\"\n",
    "# tokenizer = Smiles_tokenizer(\"<PAD>\", REGEX, \"../vocab_full.txt\", 10)\n",
    "\n",
    "# # 注意，这里一定要输入tuple（smiles1, smiles2）\n",
    "# res = tokenizer.tokenize(('[PH+][CCL2+]Clc1c(Cl)ccc(c1)[C@@H](C)[C@H](N=[N+]=[N-])C(=O)OC.C>C=O', \n",
    "#                         'Cc1cc(Br)c2c(c1)N(C(=O)O'))\n",
    "# # # res2 = tokenizer.tokenize(('[PH+][CCL2+]Clc1c(Cl)ccc(c1)[C@@H](C)[C@H](N=[N+]=[N-])C(=O)OC.C>C=O', \n",
    "# # #                           'Cc1cc(Br)c2c(c1)N(C(=O)O'))\n",
    "# print(res[0])\n",
    "# print(res[1])\n",
    "# # print(res2[0])\n",
    "# print(res2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理数据\n",
    "\n",
    "def read_data(file_path, train=True):\n",
    "    df = pd.read_csv(file_path)\n",
    "    reactant1 = df[\"Reactant1\"].tolist()\n",
    "    reactant2 = df[\"Reactant2\"].tolist()\n",
    "    product = df[\"Product\"].tolist()\n",
    "    additive = df[\"Additive\"].tolist()\n",
    "    solvent = df[\"Solvent\"].tolist()\n",
    "    if train:\n",
    "        react_yield = df[\"Yield\"].tolist()\n",
    "    else:\n",
    "        react_yield = [0 for i in range(len(reactant1))]\n",
    "    \n",
    "    # 将reactant\\additive\\solvent拼到一起，之间用.分开。product也拼到一起，用>>分开\n",
    "    input_data_list = []\n",
    "    for react1, react2, prod, addi, sol in zip(reactant1, reactant2, product, additive, solvent):\n",
    "        input_info = \".\".join([react1, react2, addi, sol])\n",
    "        input_info = \">\".join([input_info, prod])\n",
    "        input_data_list.append(input_info)\n",
    "    output = [(react, y) for react, y in zip(input_data_list, react_yield)]\n",
    "\n",
    "    # # 统计seq length\n",
    "    # seq_length = [len(i[0]) for i in output]\n",
    "    # seq_length_400 = [len(i[0]) for i in output if len(i[0])>400]\n",
    "    # print(len(seq_length_400) / len(seq_length))\n",
    "    # seq_length.sort(reverse=True)\n",
    "    # plt.plot(range(len(seq_length)), seq_length)\n",
    "    # plt.title(\"templates frequence\")\n",
    "    # plt.show()\n",
    "    return output\n",
    "# read_data(\"../dataset/round1_train_data.csv\", train=True)\n",
    "# read_data(\"../dataset/train_data_demo.csv\", train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义数据集\n",
    "class ReactionDataset(Dataset):\n",
    "    def __init__(self, data: List[Tuple[List[str], float]]):\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # input_info, react_yeild = self.data[idx]\n",
    "        # return input_info, react_yeild\n",
    "        return self.data[idx]\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    REGEX = r\"\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\\\\\|\\/|:|~|@|\\?|>|\\*|\\$|\\%[0-9]{2}|[0-9]\"\n",
    "    tokenizer = Smiles_tokenizer(\"<PAD>\", REGEX, \"../vocab_full.txt\", 400)\n",
    "    smi_list = []\n",
    "    yield_list = []\n",
    "    for i in batch:\n",
    "        smi_list.append(i[0])\n",
    "        yield_list.append(i[1])\n",
    "    tokenizer_batch = torch.tensor(tokenizer.tokenize(smi_list)[1])\n",
    "    yield_list = torch.tensor(yield_list)\n",
    "    return tokenizer_batch, yield_list\n",
    "\n",
    "# res1, res2 = collate_fn([('Clc1c(Cl)ccc(c1)[C@@H](C)[C@H](N=[N+]=[N-])C(=O)OC.C>C=O', 0.1), ('Cc1cc(Br)c2c(c1)N(C(=O)O', 0.2)])\n",
    "# print(res1)\n",
    "# print(res2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "data = read_data(\"../dataset/train_data_demo.csv\")\n",
    "# REGEX = r\"\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\\\\\|\\/|:|~|@|\\?|>|\\*|\\$|\\%[0-9]{2}|[0-9]\"\n",
    "# tokenizer = Smiles_tokenizer(\"<PAD>\", REGEX, \"../vocab_full.txt\") \n",
    "dataset = ReactionDataset(data, None)\n",
    "# 选择数据集的前N个样本进行训练\n",
    "N = 1  #int(len(dataset) * 1)  # 或者你可以设置为数据集大小的一定比例，如 int(len(dataset) * 0.1)\n",
    "subset_indices = list(range(3))\n",
    "subset_dataset = Subset(dataset, subset_indices)\n",
    "train_loader = DataLoader(subset_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "# for i, (src, trg) in enumerate(train_loader):\n",
    "#     print(src)\n",
    "#     print(trg)\n",
    "#     raise KeyError\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型\n",
    "'''\n",
    "直接采用一个transformer encoder model就好了\n",
    "'''\n",
    "class TransformerEncoderModel(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, num_heads, fnn_dim, num_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, d_model)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, \n",
    "                                                        nhead=num_heads, \n",
    "                                                        dim_feedforward=fnn_dim,\n",
    "                                                        dropout=dropout\n",
    "                                                        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.l1 = nn.Linear(d_model, 96)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.out = nn.Linear(96, 1)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src shape: [batch_size, src_len]\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        # embedded shape: [batch_size, src_len, d_model]\n",
    "        outputs = self.transformer_encoder(embedded)\n",
    "        # outputs shape: [batch_size, src_len, d_model]\n",
    "\n",
    "        # fisrt\n",
    "        z = outputs[:,0,:].squeeze(1)\n",
    "        # z shape: [bs, d_model]\n",
    "        outputs = self.out(self.relu(self.l1(z)))\n",
    "        # outputs shape: [bs, 1]\n",
    "        return outputs.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xc/miniconda3/envs/usual_torch/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "/home/xc/miniconda3/envs/usual_torch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Train Loss: 0.150\n",
      "Epoch: 02 | Train Loss: 0.081\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 54\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal running time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00melapsed_time_minute\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m minutes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 54\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 42\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# raise KeyError\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     loss_in_a_epoch \u001b[38;5;241m=\u001b[39m epoch_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_in_a_epoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 训练\n",
    "def train():\n",
    "    ## super param\n",
    "    N = 10  #int / int(len(dataset) * 1)  # 或者你可以设置为数据集大小的一定比例，如 int(len(dataset) * 0.1)\n",
    "    INPUT_DIM = 300 # src length\n",
    "    D_MODEL = 512\n",
    "    NUM_HEADS = 4\n",
    "    FNN_DIM = 1024\n",
    "    NUM_LAYERS = 6\n",
    "    DROPOUT = 0.5\n",
    "    CLIP = 1 # CLIP value\n",
    "    N_EPOCHS = 10\n",
    "    LR = 0.0001\n",
    "    \n",
    "    start_time = time.time()  # 开始计时\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # device = 'cpu'\n",
    "    data = read_data(\"../dataset/round1_train_data.csv\")\n",
    "    dataset = ReactionDataset(data)\n",
    "    subset_indices = list(range(N))\n",
    "    subset_dataset = Subset(dataset, subset_indices)\n",
    "    train_loader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "    model = TransformerEncoderModel(INPUT_DIM, D_MODEL, NUM_HEADS, FNN_DIM, NUM_LAYERS, DROPOUT)\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    optimizer = optim.SGD(model.parameters(), lr=LR)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    best_valid_loss = float('inf')\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        epoch_loss = 0\n",
    "        for i, (src, y) in enumerate(train_loader):\n",
    "            src, y = src.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(src)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "            optimizer.step()\n",
    "            # raise KeyError\n",
    "            epoch_loss += loss.item()\n",
    "            loss_in_a_epoch = epoch_loss / len(train_loader)\n",
    "        print(f'Epoch: {epoch+1:02} | Train Loss: {loss_in_a_epoch:.3f}')\n",
    "\n",
    "    # 在训练循环结束后保存模型\n",
    "    torch.save(model.state_dict(), '../model/transformer.pth')\n",
    "    end_time = time.time()  # 结束计时\n",
    "    # 计算并打印运行时间\n",
    "    elapsed_time_minute = (end_time - start_time)/60\n",
    "    print(f\"Total running time: {elapsed_time_minute:.2f} minutes\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成结果文件\n",
    "def predicit_and_make_submit_file(model_file, output_file):\n",
    "    INPUT_DIM = 300 # src length\n",
    "    D_MODEL = 512\n",
    "    NUM_HEADS = 4\n",
    "    FNN_DIM = 1024\n",
    "    NUM_LAYERS = 6\n",
    "    DROPOUT = 0.5\n",
    "    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    device = 'cpu'\n",
    "    test_data = read_data(\"../dataset/round1_test_data.csv\", train=False)\n",
    "    test_dataset = ReactionDataset(test_data)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn) \n",
    "\n",
    "    model = TransformerEncoderModel(INPUT_DIM, D_MODEL, NUM_HEADS, FNN_DIM, NUM_LAYERS, DROPOUT)\n",
    "    # 加载最佳模型\n",
    "    model.load_state_dict(torch.load(model_file))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    output_list = []\n",
    "    for i, (src, y) in enumerate(test_loader):\n",
    "        src, y = src.to(device), y.to(device)\n",
    "        output = model(src)\n",
    "        # print(output.tolist())\n",
    "        # raise KeyError\n",
    "        output_list += output\n",
    "    # print(len(output_list))\n",
    "    ans_str_lst = ['rxnid,Yield']\n",
    "    for idx,y in enumerate(output_list):\n",
    "        ans_str_lst.append(f'test{idx+1},{y:.4f}')\n",
    "    with open(output_file,'w') as fw:\n",
    "        fw.writelines('\\n'.join(ans_str_lst))\n",
    "    \n",
    "predicit_and_make_submit_file(\"../model/transformer.pth\",\n",
    "                              \"../output/submit.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
